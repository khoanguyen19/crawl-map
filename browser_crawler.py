#!/usr/bin/env python3
"""
Browser-Based Guland Crawler
Uses real browser to discover tile URL patterns for cities

Author: AI Assistant
Version: 4.0 (Pattern Discovery Only)
"""
import time
import json
import os
import random
import logging
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from map_interaction_handler import MapInteractionHandler

# Setup logging with city-based structure
def setup_city_logging(city_name):
    """Setup logging for specific city"""
    city_log_dir = f'output_browser_crawl/cities/{city_name}/logs'
    os.makedirs(city_log_dir, exist_ok=True)
    
    # Create city-specific logger
    city_logger = logging.getLogger(f'browser_crawler_{city_name}')
    city_logger.setLevel(logging.INFO)
    
    # Remove existing handlers
    for handler in city_logger.handlers[:]:
        city_logger.removeHandler(handler)
    
    # Add city-specific file handler
    city_handler = logging.FileHandler(f'{city_log_dir}/crawl_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    city_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    city_logger.addHandler(city_handler)
    
    return city_logger

# Main logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('output_browser_crawl/browser_pattern_discovery.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BrowserGulandCrawler:
    def __init__(self, headless=False):
        self.driver = None
        self.headless = headless
        self.map_handler = None
        self.discovered_data = {
            'all_locations': [],
            'tile_servers': set(),
            'tile_patterns': set(),
            'success_count': 0,
            'failure_count': 0
        }
        
        # Create organized output directories
        self.setup_output_structure()
        
        self.test_locations = [
            # ("TP Há»“ ChÃ­ Minh", 10.8231, 106.6297, "soi-quy-hoach/tp-ho-chi-minh"),
            ("Äá»“ng Nai", 11.0686, 107.1676, "soi-quy-hoach/dong-nai"),
            ("BÃ  Rá»‹a - VÅ©ng TÃ u", 10.5417, 107.2431, "soi-quy-hoach/ba-ria-vung-tau"),
            ("An Giang", 10.3889, 105.4359, "soi-quy-hoach/an-giang"),
            # ("Báº¯c Giang", 21.2731, 106.1946, "soi-quy-hoach/bac-giang"),
            # ("Báº¯c Káº¡n", 22.1474, 105.8348, "soi-quy-hoach/bac-kan"),
            # ("Báº¡c LiÃªu", 9.2515, 105.7244, "soi-quy-hoach/bac-lieu"),
            # ("Báº¯c Ninh", 21.1861, 106.0763, "soi-quy-hoach/bac-ninh"),
            # ("Báº¿n Tre", 10.2433, 106.3756, "soi-quy-hoach/ben-tre"),
            # ("BÃ¬nh DÆ°Æ¡ng", 11.3254, 106.4770, "soi-quy-hoach/binh-duong"),
            # ("BÃ¬nh PhÆ°á»›c", 11.7511, 106.7234, "soi-quy-hoach/binh-phuoc"),
            # ("BÃ¬nh Thuáº­n", 11.0904, 108.0721, "soi-quy-hoach/binh-thuan"),
            # ("BÃ¬nh Äá»‹nh", 13.7757, 109.2219, "soi-quy-hoach/binh-dinh"),
            # ("CÃ  Mau", 9.1769, 105.1524, "soi-quy-hoach/ca-mau")
            # ("Cáº§n ThÆ¡", 10.0452, 105.7469, "soi-quy-hoach/can-tho"),
            # ("Cao Báº±ng", 22.6666, 106.2639, "soi-quy-hoach/cao-bang"),
            # ("Gia Lai", 13.8078, 108.1094, "soi-quy-hoach/gia-lai"),
            # ("HÃ  Nam", 20.5835, 105.9230, "soi-quy-hoach/ha-nam"),
            # ("HÃ  Giang", 22.8025, 104.9784, "soi-quy-hoach/ha-giang"),
            # ("HÃ  Ná»™i", 21.0285, 105.8542, "soi-quy-hoach/ha-noi"),
            # ("HÃ  TÄ©nh", 18.3560, 105.9069, "soi-quy-hoach/ha-tinh"),
            # ("Háº£i PhÃ²ng", 20.8449, 106.6881, "soi-quy-hoach/hai-phong"),
            # ("Háº­u Giang", 9.7571, 105.6412, "soi-quy-hoach/hau-giang"),
            # ("HÃ²a BÃ¬nh", 20.8156, 105.3373, "soi-quy-hoach/hoa-binh"),
            # ("HÆ°ng YÃªn", 20.6464, 106.0511, "soi-quy-hoach/hung-yen"),
            # ("KhÃ¡nh HÃ²a", 12.2388, 109.1967, "soi-quy-hoach/khanh-hoa"),
            # ("KiÃªn Giang", 10.0125, 105.0808, "soi-quy-hoach/kien-giang"),
            # ("Kon Tum", 14.3497, 108.0005, "soi-quy-hoach/kon-tum"),
            # ("Lai ChÃ¢u", 22.3856, 103.4707, "soi-quy-hoach/lai-chau"),
            # ("LÃ¢m Äá»“ng", 11.5753, 108.1429, "soi-quy-hoach/lam-dong"),
            # ("Láº¡ng SÆ¡n", 21.8537, 106.7610, "soi-quy-hoach/lang-son"),
            # ("LÃ o Cai", 22.4809, 103.9755, "soi-quy-hoach/lao-cai"),
            # ("Long An", 10.6957, 106.2431, "soi-quy-hoach/long-an"),
            # ("Nam Äá»‹nh", 20.4341, 106.1675, "soi-quy-hoach/nam-dinh"),
            # ("Nghá»‡ An", 18.6745, 105.6905, "soi-quy-hoach/nghe-an"),
            # ("Ninh BÃ¬nh", 20.2506, 105.9744, "soi-quy-hoach/ninh-binh"),
            # ("Ninh Thuáº­n", 11.5645, 108.9899, "soi-quy-hoach/ninh-thuan"),
            # ("PhÃº Thá»", 21.4208, 105.2045, "soi-quy-hoach/phu-tho"),
            # ("PhÃº YÃªn", 13.0882, 109.0929, "soi-quy-hoach/phu-yen"),
            # ("Quáº£ng BÃ¬nh", 17.4809, 106.6238, "soi-quy-hoach/quang-binh"),
            # ("Quáº£ng Nam", 15.5394, 108.0191, "soi-quy-hoach/quang-nam"),
            # ("Quáº£ng NgÃ£i", 15.1214, 108.8044, "soi-quy-hoach/quang-ngai"),
            # ("Quáº£ng Ninh", 21.0064, 107.2925, "soi-quy-hoach/quang-ninh"),
            # ("Quáº£ng Trá»‹", 16.7404, 107.1854, "soi-quy-hoach/quang-tri"),
            # ("SÃ³c TrÄƒng", 9.6002, 105.9800, "soi-quy-hoach/soc-trang"),
            # ("SÆ¡n La", 21.3256, 103.9188, "soi-quy-hoach/son-la"),
            # ("TÃ¢y Ninh", 11.3100, 106.0989, "soi-quy-hoach/tay-ninh"),
            # ("ThÃ¡i BÃ¬nh", 20.4500, 106.3400, "soi-quy-hoach/thai-binh"),
            # ("ThÃ¡i NguyÃªn", 21.5944, 105.8480, "soi-quy-hoach/thai-nguyen"),
            # ("Thanh HÃ³a", 19.8069, 105.7851, "soi-quy-hoach/thanh-hoa"),
            # ("Thá»«a ThiÃªn Huáº¿", 16.4674, 107.5905, "soi-quy-hoach/thua-thien-hue"),
            # ("Tiá»n Giang", 10.4493, 106.3420, "soi-quy-hoach/tien-giang"),
            # ("TrÃ  Vinh", 9.9477, 106.3524, "soi-quy-hoach/tra-vinh"),
            # ("TuyÃªn Quang", 21.8267, 105.2280, "soi-quy-hoach/tuyen-quang"),
            # ("VÄ©nh Long", 10.2397, 105.9571, "soi-quy-hoach/vinh-long"),
            # ("VÄ©nh PhÃºc", 21.3609, 105.6049, "soi-quy-hoach/vinh-phuc"),
            # ("YÃªn BÃ¡i", 21.7168, 104.8986, "soi-quy-hoach/yen-bai"),
            # ("ÄÃ  Náºµng", 16.0544563, 108.0717219, "soi-quy-hoach/da-nang"),
            # ("Äáº¯k Láº¯k", 12.7100, 108.2378, "soi-quy-hoach/dak-lak"),
            # ("Äáº¯k NÃ´ng", 12.2646, 107.6098, "soi-quy-hoach/dak-nong"),
            # ("Äiá»‡n BiÃªn", 21.3847, 103.0175, "soi-quy-hoach/dien-bien"),
            # ("Äá»“ng ThÃ¡p", 10.4938, 105.6881, "soi-quy-hoach/dong-thap")
        ]
        
        # Network request tracking
        self.captured_requests = []

    def setup_output_structure(self):
        """Setup organized directory structure"""
        base_dirs = [
            'output_browser_crawl',
            'output_browser_crawl/cities',
            'output_browser_crawl/patterns',
            'output_browser_crawl/reports',
            'output_browser_crawl/screenshots'
        ]
        
        for dir_path in base_dirs:
            os.makedirs(dir_path, exist_ok=True)
        
        logger.info("ğŸ“ Output directory structure created")

    def setup_city_directories(self, city_name):
        """Setup directories for specific city"""
        clean_city_name = city_name.replace(' ', '_').replace('TP ', '')
        city_dirs = [
            f'output_browser_crawl/cities/{clean_city_name}',
            f'output_browser_crawl/cities/{clean_city_name}/logs',
            f'output_browser_crawl/cities/{clean_city_name}/reports',
            f'output_browser_crawl/cities/{clean_city_name}/screenshots',
            f'output_browser_crawl/cities/{clean_city_name}/network_logs'
        ]
        
        for dir_path in city_dirs:
            os.makedirs(dir_path, exist_ok=True)
        
        return clean_city_name

    def setup_driver(self):
        """Setup Chrome driver with network logging enabled"""
        logger.info("ğŸš€ Setting up Chrome driver for pattern discovery...")
        
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument("--headless")
        
        # Enable logging for network requests
        chrome_options.add_argument("--enable-logging")
        chrome_options.add_argument("--log-level=0")
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        
        # Enable performance logging to capture network requests
        chrome_options.set_capability('goog:loggingPrefs', {
            'performance': 'ALL',
            'network': 'ALL'
        })
        
        # Make browser look realistic
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # User agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        )
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # Execute script to remove webdriver property
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # Initialize map interaction handler
            self.map_handler = MapInteractionHandler(self.driver)
            
            logger.info("âœ… Chrome driver setup successful for pattern discovery")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to setup Chrome driver: {e}")
            return False

    def start_network_capture(self):
        """Start capturing network requests"""
        logger.info("ğŸ“¡ Starting network capture for pattern discovery...")
        self.captured_requests = []
        
        # Clear existing performance logs
        self.driver.get_log('performance')

    def get_network_requests(self):
        """Get all network requests from performance log with enhanced filtering"""
        logs = self.driver.get_log('performance')
        requests = []
        
        for log in logs:
            try:
                message = json.loads(log['message'])
                
                # Capture both response received and request sent
                if message['message']['method'] in ['Network.responseReceived', 'Network.requestWillBeSent']:
                    if message['message']['method'] == 'Network.responseReceived':
                        response = message['message']['params']['response']
                        url = response['url']
                        status = response.get('status', 0)
                    elif message['message']['method'] == 'Network.requestWillBeSent':
                        request = message['message']['params']['request']
                        url = request['url']
                        status = 'pending'
                    
                    requests.append({
                        'url': url,
                        'timestamp': log['timestamp'],
                        'status': status,
                        'method': message['message']['method']
                    })
            except (KeyError, json.JSONDecodeError) as e:
                # Skip malformed log entries
                continue
        
        # Remove duplicates and sort by timestamp
        unique_urls = set()
        unique_requests = []
        
        for request in sorted(requests, key=lambda x: x['timestamp']):
            if request['url'] not in unique_urls:
                unique_urls.add(request['url'])
                unique_requests.append(request)
        
        logger.info(f"ğŸ“¡ Total unique requests captured: {len(unique_requests)}")
        
        return unique_requests

    def extract_tile_urls(self, requests):
        """Enhanced tile URL extraction with better pattern matching"""
        tile_urls = []
        tile_patterns = set()
        
        logger.info(f"ğŸ” Analyzing {len(requests)} requests for tile patterns...")
        
        for request in requests:
            url = request['url']
            
            # Enhanced tile detection patterns
            tile_indicators = [
                '.png', '.jpg', '.jpeg', '.webp', '.tiff', '.tif'
            ]
            
            # Check if this looks like a tile URL
            if any(ext in url.lower() for ext in tile_indicators):
                logger.info(f"ğŸ” Checking potential tile URL: {url}")
                
                # Pattern 1: Standard /z/x/y.ext
                tile_pattern = re.search(r'/(\d+)/(\d+)/(\d+)\.(png|jpg|jpeg|webp|tiff|tif)', url, re.IGNORECASE)
                
                if tile_pattern:
                    zoom, x, y, ext = tile_pattern.groups()
                    logger.info(f"âœ… Found tile: z={zoom}, x={x}, y={y}, ext={ext}")
                    
                    # Extract base URL pattern
                    base_url = url.split(f'/{zoom}/{x}/{y}.{ext}')[0]
                    pattern = f"{base_url}/{{z}}/{{x}}/{{y}}.{ext}"
                    tile_patterns.add(pattern)
                    
                    tile_urls.append({
                        'url': url,
                        'base_url': base_url,
                        'zoom': int(zoom),
                        'x': int(x),
                        'y': int(y),
                        'format': ext.lower(),
                        'status': request['status'],
                        'pattern': pattern
                    })
                else:
                    # Pattern 2: Alternative formats
                    alt_patterns = [
                        r'/(\d+)_(\d+)_(\d+)\.(png|jpg|jpeg|webp)',
                        r'/(\d+)-(\d+)-(\d+)\.(png|jpg|jpeg|webp)',
                        r'/tile_(\d+)_(\d+)_(\d+)\.(png|jpg|jpeg|webp)'
                    ]
                    
                    for alt_pattern in alt_patterns:
                        alt_match = re.search(alt_pattern, url, re.IGNORECASE)
                        if alt_match:
                            zoom, x, y, ext = alt_match.groups()
                            logger.info(f"âœ… Found alternative tile format: z={zoom}, x={x}, y={y}")
                            
                            # Create pattern
                            base_url = re.sub(alt_pattern, '', url)
                            pattern = f"{base_url}/{{z}}_{{x}}_{{y}}.{ext}"
                            tile_patterns.add(pattern)
                            
                            tile_urls.append({
                                'url': url,
                                'base_url': base_url,
                                'zoom': int(zoom),
                                'x': int(x),
                                'y': int(y),
                                'format': ext.lower(),
                                'status': request['status'],
                                'pattern': pattern
                            })
                            break
        
        logger.info(f"ğŸ¯ Extracted {len(tile_urls)} tile URLs")
        logger.info(f"ğŸ¯ Found {len(tile_patterns)} unique patterns")
        
        # Log found patterns for debugging
        for pattern in tile_patterns:
            logger.info(f"ğŸ¯ Pattern: {pattern}")
        
        return tile_urls, tile_patterns

    def systematic_zoom_coverage_for_patterns(self, location_name, lat, lng, duration_per_zoom=20):
        """Systematically cover all zoom levels 10-18 to discover patterns only"""
        logger.info(f"ğŸ¯ Starting systematic pattern discovery for {location_name}")
        
        zoom_levels = list(range(10, 19))  # 10 to 18
        all_discovered_patterns = set()
        
        for zoom_index, zoom in enumerate(zoom_levels):
            logger.info(f"ğŸ” Discovering patterns at zoom level {zoom} ({zoom_index+1}/{len(zoom_levels)})")
            
            try:
                # Clear network logs for this zoom level
                self.driver.get_log('performance')
                
                # Set specific zoom level
                zoom_success = self.map_handler.set_map_zoom(zoom)
                if not zoom_success:
                    self.map_handler.simulate_zoom_interaction(zoom)
                
                # Wait for tiles to load at this zoom
                time.sleep(3)
                
                # Perform coverage at this zoom using map handler
                action_count = self.map_handler.comprehensive_map_coverage(zoom, duration_per_zoom)
                
                # Get patterns discovered during this coverage
                requests = self.get_network_requests()
                tile_urls, zoom_patterns = self.extract_tile_urls(requests)
                
                # Add new patterns to our collection
                all_discovered_patterns.update(zoom_patterns)
                
                if zoom_patterns:
                    logger.info(f"âœ… Zoom {zoom}: Discovered {len(zoom_patterns)} patterns")
                    for pattern in zoom_patterns:
                        logger.info(f"  ğŸ“‹ Pattern: {pattern}")
                else:
                    logger.warning(f"âš ï¸ Zoom {zoom}: No patterns discovered")
                
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"âŒ Error at zoom {zoom}: {e}")
                continue
        
        logger.info(f"ğŸ‰ Pattern discovery complete: {len(all_discovered_patterns)} unique patterns")
        return list(all_discovered_patterns)

    def crawl_location_for_patterns(self, location_name, lat, lng, path):
        """Crawl location to discover tile patterns only"""
        clean_city_name = self.setup_city_directories(location_name)
        city_logger = setup_city_logging(clean_city_name)
        
        city_logger.info(f"ğŸ¯ PATTERN DISCOVERY: {location_name}")
        city_logger.info("=" * 60)
        
        try:
            # Navigate to location page
            if not self.navigate_to_location_page(location_name, path):
                return None
            
            # Take initial screenshot
            screenshot_path = f"output_browser_crawl/cities/{clean_city_name}/screenshots/initial_{datetime.now().strftime('%H%M%S')}.png"
            self.driver.save_screenshot(screenshot_path)
            city_logger.info(f"ğŸ“¸ Initial screenshot: {screenshot_path}")
            
            # Detect city boundaries
            bounds = self.map_handler.detect_city_boundaries(location_name)
            
            # Start pattern discovery across all zoom levels
            discovered_patterns = self.systematic_zoom_coverage_for_patterns(location_name, lat, lng)
            
            # Extract unique servers
            tile_servers = set()
            for pattern in discovered_patterns:
                server = self.extract_server_from_url(pattern)
                tile_servers.add(server)
            
            # Update global discovered data
            self.discovered_data['tile_patterns'].update(discovered_patterns)
            self.discovered_data['tile_servers'].update(tile_servers)
            self.discovered_data['success_count'] += 1
            
            location_result = {
                'location_name': location_name,
                'clean_name': clean_city_name,
                'timestamp': datetime.now().isoformat(),
                'coordinates': {
                    'lat': lat,
                    'lng': lng
                },
                'discovered_patterns': list(discovered_patterns),
                'tile_servers': list(tile_servers),
                'bounds': bounds,
                'pattern_count': len(discovered_patterns),
                'server_count': len(tile_servers)
            }
            
            # Save city-specific pattern report
            self.save_city_pattern_report(clean_city_name, location_result)
            
            city_logger.info(f"ğŸ‰ PATTERN DISCOVERY COMPLETE: {location_name}")
            city_logger.info(f"ğŸ“Š Patterns discovered: {len(discovered_patterns)}")
            city_logger.info(f"ğŸ“Š Servers found: {len(tile_servers)}")
            
            return location_result
            
        except Exception as e:
            city_logger.error(f"âŒ Error in pattern discovery: {e}")
            self.discovered_data['failure_count'] += 1
            return None

    def save_city_pattern_report(self, clean_city_name, location_result):
        """Save detailed pattern report for city"""
        
        # Save JSON report
        json_path = f"output_browser_crawl/cities/{clean_city_name}/reports/patterns_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(location_result, f, indent=2, ensure_ascii=False)
        
        # Save text report
        text_report = f"""# TILE PATTERN DISCOVERY REPORT: {location_result['location_name']}
Generated: {location_result['timestamp']}
Method: Browser-based systematic zoom coverage (10-18)

## ğŸ“ LOCATION INFO
â€¢ Name: {location_result['location_name']}
â€¢ Coordinates: {location_result['coordinates']['lat']}, {location_result['coordinates']['lng']}
â€¢ Bounds: {location_result['bounds']}

## ğŸ“Š DISCOVERY SUMMARY
â€¢ Patterns discovered: {location_result['pattern_count']}
â€¢ Tile servers: {location_result['server_count']}

## ğŸ¯ DISCOVERED PATTERNS
"""
        
        for pattern in location_result['discovered_patterns']:
            text_report += f"â€¢ {pattern}\n"
        
        text_report += f"\n## ğŸ—ºï¸ TILE SERVERS\n"
        for server in location_result['tile_servers']:
            text_report += f"â€¢ {server}\n"
        
        text_path = f"output_browser_crawl/cities/{clean_city_name}/reports/patterns_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        logger.info(f"ğŸ’¾ City pattern report saved: {clean_city_name}")

    def extract_server_from_url(self, url):
        """Extract server base URL from tile URL"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"

    def generate_final_patterns_report(self):
        """Generate final comprehensive patterns report"""
        logger.info("ğŸ“Š Generating final patterns discovery report...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'crawler': 'Browser Pattern Discovery Crawler v4.0',
            'method': 'Systematic zoom coverage 10-18 for pattern discovery',
            'summary': {
                'total_attempted': len(self.test_locations),
                'total_successful': self.discovered_data['success_count'],
                'total_failed': self.discovered_data['failure_count'],
                'success_rate': (self.discovered_data['success_count'] / len(self.test_locations) * 100) if len(self.test_locations) > 0 else 0,
                'unique_tile_patterns': len(self.discovered_data['tile_patterns']),
                'tile_servers': len(self.discovered_data['tile_servers'])
            },
            'tile_patterns': list(self.discovered_data['tile_patterns']),
            'tile_servers': list(self.discovered_data['tile_servers']),
            'successful_locations': self.discovered_data['all_locations']
        }
        
        # Save comprehensive report
        final_report_path = 'output_browser_crawl/reports/final_patterns_report.json'
        with open(final_report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Generate text summary
        text_report = f"""# GULAND TILE PATTERNS DISCOVERY REPORT
Generated: {report['timestamp']}
Method: Browser-based systematic pattern discovery

## ğŸ“Š DISCOVERY SUMMARY
â€¢ Total Locations Attempted: {report['summary']['total_attempted']}
â€¢ Successful Discoveries: {report['summary']['total_successful']}
â€¢ Success Rate: {report['summary']['success_rate']:.1f}%
â€¢ Unique Tile Patterns: {report['summary']['unique_tile_patterns']}
â€¢ Tile Servers: {report['summary']['tile_servers']}

## ğŸ¯ ALL DISCOVERED TILE PATTERNS
"""
        
        for pattern in report['tile_patterns']:
            text_report += f"â€¢ {pattern}\n"
        
        text_report += f"\n## ğŸ—ºï¸ TILE SERVERS\n"
        for server in report['tile_servers']:
            text_report += f"â€¢ {server}\n"
        
        # Add location breakdown
        text_report += f"\n## ğŸ“ LOCATION BREAKDOWN\n"
        for location in report['successful_locations']:
            text_report += f"### {location['location_name']}\n"
            text_report += f"â€¢ Patterns found: {location['pattern_count']}\n"
            text_report += f"â€¢ Servers: {location['server_count']}\n"
            for pattern in location['discovered_patterns']:
                text_report += f"  - {pattern}\n"
            text_report += "\n"
        
        text_report_path = 'output_browser_crawl/reports/final_patterns_summary.txt'
        with open(text_report_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        
        logger.info("âœ… Final patterns report generated")
        
        # Print summary to console
        print(f"\nğŸ‰ PATTERN DISCOVERY COMPLETED!")
        print("=" * 60)
        print(f"ğŸ“Š Discovery Results:")
        print(f"  â€¢ Locations processed: {report['summary']['total_successful']}/{report['summary']['total_attempted']}")
        print(f"  â€¢ Success rate: {report['summary']['success_rate']:.1f}%")
        print(f"  â€¢ Unique tile patterns: {report['summary']['unique_tile_patterns']}")
        print(f"  â€¢ Tile servers: {report['summary']['tile_servers']}")
        print(f"\nğŸ“ Reports saved to: output_browser_crawl/reports/")
        print(f"ğŸ“ City-specific data: output_browser_crawl/cities/")
        print(f"\nğŸš€ Next step: Run pattern_based_tile_crawler.py to download tiles")
        
        return report

    def run_pattern_discovery_crawl(self, max_hours=2):
        """Run complete pattern discovery crawl"""
        logger.info("ğŸš€ STARTING PATTERN DISCOVERY CRAWL")
        logger.info("=" * 70)
        
        crawl_start_time = time.time()
        max_crawl_time = max_hours * 3600
        
        try:
            if not self.setup_driver():
                return None
            
            logger.info(f"â° Time limit set to {max_hours} hours")
            logger.info("ğŸŒ Warming up session...")
            self.driver.get("https://guland.vn/")
            time.sleep(random.uniform(3, 6))
            
            for i, (location_name, lat, lng, path) in enumerate(self.test_locations, 1):
                # Check global timeout
                elapsed_time = time.time() - crawl_start_time
                remaining_time = max_crawl_time - elapsed_time
                
                if remaining_time < 300:  # Less than 5 minutes remaining
                    logger.warning(f"â° Time limit reached: {elapsed_time/3600:.1f}h/{max_hours}h")
                    break
                
                logger.info(f"\nğŸŒ PROCESSING {i}/{len(self.test_locations)}: {location_name}")
                logger.info(f"â±ï¸ Elapsed: {elapsed_time/3600:.1f}h, Remaining: {remaining_time/3600:.1f}h")
                logger.info("=" * 60)
                
                location_start_time = time.time()
                
                # Discover patterns for this location
                location_info = self.crawl_location_for_patterns(location_name, lat, lng, path)
                
                location_elapsed = time.time() - location_start_time
                logger.info(f"â±ï¸ {location_name} processed in {location_elapsed:.1f}s")
                
                if location_info:
                    self.discovered_data['all_locations'].append(location_info)
                    logger.info(f"âœ… {location_name}: {location_info['pattern_count']} patterns discovered")
                else:
                    logger.warning(f"âš ï¸ Failed to discover patterns for {location_name}")
                
                # Delay between locations
                if i < len(self.test_locations):
                    delay = min(30, max(5, remaining_time * 0.02))
                    logger.info(f"â³ Waiting {delay:.1f}s before next location...")
                    time.sleep(delay)
            
            # Generate final comprehensive report
            final_report = self.generate_final_patterns_report()
            return final_report
            
        except Exception as e:
            logger.error(f"âŒ Pattern discovery crawl failed: {e}")
            return None
            
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ğŸ”š Browser closed")
            
            total_elapsed = time.time() - crawl_start_time
            logger.info(f"â±ï¸ Total crawl time: {total_elapsed/3600:.1f} hours")

    def navigate_to_location_page(self, location_name, path):
        """Navigate to planning page of location"""
        logger.info(f"ğŸŒ Navigating to {location_name} planning page...")
        
        try:
            url = f"https://guland.vn/{path}"
            logger.info(f"ğŸ”— Opening URL: {url}")
            self.driver.get(url)
            
            # Wait for page to load
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Wait for map/JS to initialize
            time.sleep(random.uniform(5, 8))
            
            logger.info(f"âœ… Successfully navigated to {location_name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to navigate to {location_name}: {e}")
            return False

def main():
    """Main function for pattern discovery"""
    print("ğŸ‡»ğŸ‡³ GULAND TILE PATTERN DISCOVERY")
    print("Discovers tile URL patterns for Vietnamese cities")
    print("=" * 60)
    
    # Time limit configuration
    print("\nTime limit options:")
    print("1. Quick (1 hour) - 2-3 cities")
    print("2. Standard (2 hours) - 5-7 cities") 
    print("3. Extended (4 hours) - 10-15 cities")
    print("4. Full (8 hours) - All cities")
    
    time_choice = input("Choose time limit (1-4, default 2): ").strip()
    time_map = {'1': 1, '2': 2, '3': 4, '4': 8}
    max_hours = time_map.get(time_choice, 2)
    
    # Browser mode
    headless_input = input("Run headless browser? (y/N): ").lower().strip()
    headless = headless_input in ['y', 'yes']
    
    # Create crawler
    crawler = BrowserGulandCrawler(headless=headless)
    
    print(f"\nğŸ¯ DISCOVERY CONFIGURATION:")
    print(f"â° Time limit: {max_hours} hours")
    print(f"ğŸ“Š Max locations: {len(crawler.test_locations)}")
    print(f"ğŸ–¥ï¸ Headless: {'Yes' if headless else 'No'}")
    print(f"ğŸ¯ Goal: Discover tile URL patterns only")
    
    confirm = input("\nStart pattern discovery? (Y/n): ").lower().strip()
    if confirm in ['n', 'no']:
        print("Cancelled.")
        return
    
    try:
        results = crawler.run_pattern_discovery_crawl(max_hours=max_hours)
        
        if results and results['summary']['total_successful'] > 0:
            print("\nâœ… SUCCESS! Discovered tile patterns")
            print(f"ğŸ¯ Found {results['summary']['unique_tile_patterns']} unique patterns")
            print(f"ğŸ—ºï¸ From {results['summary']['tile_servers']} different servers")
            print("ğŸ“ Check 'output_browser_crawl/' for detailed results")
            print("\nğŸš€ Next: Run pattern_based_tile_crawler.py to download tiles")
        else:
            print("\nâš ï¸ No patterns discovered")
            print("ğŸ’¡ Try increasing time limit or check target locations")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ Discovery stopped by user")
    except Exception as e:
        print(f"\nâŒ Discovery failed: {e}")
        print("ğŸ’¡ Check logs for details")

if __name__ == "__main__":
    main()